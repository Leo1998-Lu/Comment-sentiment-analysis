{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ocemotion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OysFd6aM8Gy",
        "outputId": "f595db24-2871-4d13-bbf8-2cc120f2aef5"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jan  5 11:13:47 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiY8JC4s7dmg",
        "outputId": "7cc01740-5593-43c3-c6ac-19bab051b268"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m67y-0P7_Do"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Colab_NLP/tianchi-multi-task-nlp\"\n",
        "\n",
        "nb_path = '/content/drive/MyDrive/Colab_NLP/tianchi-multi-task-nlp/'\n",
        "sys.path.insert(0,nb_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-70uHuBJQwNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8c355f-38cc-4961-fd68-9897a4502e8a"
      },
      "source": [
        "pip install keras-bert #keras tensorflow pycaret fasttext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.19.4)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (3.13)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.4.3->keras-bert) (1.15.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34145 sha256=a5ed7e28ffe7763e3fb9f7c4a46b2849d1b16504d4514d06f399fb145fc3d4db\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12944 sha256=ac019c63edadd9c31247a11a9131f327850465baa24f586d2352d32671031d53\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=2ae0729243d120f655c4b632b340b985835fc9a713649f054da1a972e7e52ed9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=5d9a916b0e28d86db6879a3179a012c1e4d1dacab07b6f0d52b767eadcc8233c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5267 sha256=2b5d6b972f22f98e2b38852b653d39763d56d3fbfeb4874c996229e36fc30679\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5625 sha256=e889be9ac9c027c87105dcc7dc1f6f69fe22eb888bd70b7dcfbd008218ec2f5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=b445fe1635470fc5a378ac5022d8c66cb4233fa6cee7dd4580b58330594b3b07\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=94bb4342bb61338e0cf586401367c801c4e0001a9591756c0a5475ec5a1e0df5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LS6QV2JCYlm"
      },
      "source": [
        "import pandas as pd\n",
        "import codecs, gc\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPy10_hu55QB"
      },
      "source": [
        "#将ocnli中content1+content2作为content输入\n",
        "#times_train = pd.read_csv(path + '/tianchi_datasets/TNEWS/total.csv',  sep='\\t', header=None, names=('id', 'content', 'label')).astype(str)\n",
        "ocemo_train = pd.read_csv(path + '/tianchi_datasets/OCEMOTION/total.csv',sep='\\t', header=None, names=('id', 'content', 'label')).astype(str)\n",
        "# ocnli_train = pd.read_csv(path + '/tianchi_datasets/OCNLI/total.csv',  sep='\\t', header=None, names=('id', 'content1', 'content2', 'label')).astype(str)\n",
        "# ocnli_train['content'] = ocnli_train['content1'] + ocnli_train['content2']\n",
        "#/content/drive/MyDrive/Colab_NLP/tianchi-multi-task-nlp/TEST/ocemotion_test_B.csv\n",
        "#times_testa = pd.read_csv(path + '/tianchi_datasets/TNEWS/test.csv',  sep='\\t', header=None, names=('id', 'content')).astype(str)\n",
        "ocemo_testa = pd.read_csv(path + '/TEST/ocemotion_test_B.csv',sep='\\t', header=None, names=('id', 'content')).astype(str)\n",
        "# ocnli_testa = pd.read_csv(path + '/tianchi_datasets/OCNLI/test.csv',  sep='\\t', header=None, names=('id', 'content1', 'content2')).astype(str)\n",
        "# ocnli_testa['content'] = ocnli_testa['content1'] + ocnli_testa['content2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "basDcr_oDKu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1dfa19-868f-41b8-d8a1-72dab2d90979"
      },
      "source": [
        "#构造输入、标签数据\n",
        "train_df = ocemo_train#pd.concat([times_train, ocemo_train, ocnli_train[['id','content', 'label']]], axis=0).copy()\n",
        "testa_df = ocemo_testa#pd.concat([times_testa, ocemo_testa, ocnli_testa[['id', 'content']]], axis=0).copy()\n",
        "#label需要从0开始\n",
        "encode_label = LabelEncoder()\n",
        "train_df['label'] = encode_label.fit_transform(train_df['label'].apply(str))\n",
        "\n",
        "#train_df[['label']].to_csv(path + \"/subs/trnlabel.csv\", index=None)\n",
        "\n",
        "###\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
        "\n",
        "X_trn = pd.DataFrame()\n",
        "X_val = pd.DataFrame()\n",
        "\n",
        "for train_index, test_index in skf.split(train_df.copy(), train_df['label']):\n",
        "  X_trn, X_val = train_df.iloc[train_index], train_df.iloc[test_index]\n",
        "  break\n",
        "###\n",
        "\n",
        "subs = testa_df[['id']]\n",
        "testa_df = testa_df[['content']]\n",
        "\n",
        "#训练数据、测试数据和标签转化为模型输入格式\n",
        "n_cls = len( train_df['label'].unique() )\n",
        "TRN_LIST = []\n",
        "for data_row in X_trn.iloc[:].itertuples():\n",
        "  TRN_LIST.append((data_row.content, to_categorical(data_row.label, n_cls)))\n",
        "TRN_LIST = np.array(TRN_LIST)\n",
        "\n",
        "VAL_LIST = []\n",
        "for data_row in X_val.iloc[:].itertuples():\n",
        "  VAL_LIST.append((data_row.content, to_categorical(data_row.label, n_cls)))\n",
        "VAL_LIST = np.array(VAL_LIST)\n",
        " \n",
        "DATA_LIST_TEST = []\n",
        "for data_row in testa_df.iloc[:].itertuples():\n",
        "  DATA_LIST_TEST.append((data_row.content, to_categorical(0, n_cls)))\n",
        "DATA_LIST_TEST = np.array(DATA_LIST_TEST)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq1v4y90e41k",
        "outputId": "29c7c271-f2b7-4c53-f184-ab8fd997c693"
      },
      "source": [
        "print(train_df.shape, testa_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35315, 3) (1500, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXM7H4D78vJF",
        "outputId": "f2487f31-07e8-4235-fd12-a8745cc6695e"
      },
      "source": [
        "!unzip /content/drive/MyDrive/Colab_NLP/chinese_roberta_wwm_large_ext_L-24_H-1024_A-16.zip "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab_NLP/chinese_roberta_wwm_large_ext_L-24_H-1024_A-16.zip\n",
            "  inflating: bert_config.json        \n",
            "  inflating: bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: bert_model.ckpt.index   \n",
            "  inflating: bert_model.ckpt.meta    \n",
            "  inflating: vocab.txt               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr7ysAdHKJXU"
      },
      "source": [
        "#预训练好的模型\n",
        "path_bert = '/content/chinese_roberta_wwm_large_ext_L-24_H-1024_A-16/'\n",
        "config_path = '/content/bert_config.json'\n",
        "dict_path  = '/content/vocab.txt'\n",
        "checkpoint_path ='/content/bert_model.ckpt'\n",
        "\n",
        "#一些自定义参数\n",
        "er_patience = 2 #early_stopping patience\n",
        "lr_patience = 2 #ReduceLROnPlateau patience\n",
        "max_epochs = 3 #epochs\n",
        "lr_rate = 2e-6 #learning rate\n",
        "batch_sz = 6 #batch_size\n",
        "maxlen = 158  #设置序列长度为，要保证序列长度不超过512 ??\n",
        "nfold = 5\n",
        "'''\n",
        "#0.58分数\n",
        "er_patience = 2 #early_stopping patience\n",
        "lr_patience = 3 #ReduceLROnPlateau patience\n",
        "max_epochs = 3 #epochs\n",
        "lr_rate = 1e-5 #learning rate\n",
        "batch_sz = 16 #batch_size\n",
        "maxlen = 64  #设置序列长度为，要保证序列长度不超过512 ??\n",
        "'''\n",
        "#将词表中的词编号转换为字典\n",
        "token_dict = {}\n",
        "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
        "  for line in reader:\n",
        "    token = line.strip()\n",
        "    token_dict[token] = len(token_dict)\n",
        "\n",
        "#重写tokenizer        \n",
        "class OurTokenizer(Tokenizer):\n",
        "  def _tokenize(self, text):\n",
        "    R = []\n",
        "    for c in text:\n",
        "      if c in self._token_dict:\n",
        "        R.append(c)\n",
        "      elif self._is_space(c):\n",
        "        R.append('[unused1]')  # 用[unused1]来表示空格类字符\n",
        "      else:\n",
        "        R.append('[UNK]')  # 不在列表的字符用[UNK]表示\n",
        "    return R\n",
        "\n",
        "tokenizer = OurTokenizer(token_dict)\n",
        " \n",
        "#让每条文本的长度相同，用0填充\n",
        "def seq_padding(X, padding=0):\n",
        "  L = [len(x) for x in X]\n",
        "  ML = max(L)\n",
        "  return np.array([  np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X  ])\n",
        "  \n",
        "#data_generator只是一种为了节约内存的数据方式\n",
        "class data_generator:\n",
        "  global batch_sz\n",
        "  def __init__(self, data, batch_size=batch_sz, shuffle=True):#此处修改batch_size\n",
        "    self.data = data\n",
        "    self.batch_size = batch_size\n",
        "    self.shuffle = shuffle\n",
        "    self.steps = len(self.data) // self.batch_size\n",
        "    if len(self.data) % self.batch_size != 0:\n",
        "      self.steps += 1\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.steps\n",
        "\n",
        "  def __iter__(self):\n",
        "    while True:\n",
        "      idxs = list(range(len(self.data)))\n",
        "      if self.shuffle:\n",
        "        np.random.shuffle(idxs)\n",
        "      X1, X2, Y = [], [], []\n",
        "      for i in idxs:\n",
        "        d = self.data[i]\n",
        "        text = d[0][:maxlen]\n",
        "        x1, x2 = tokenizer.encode(first=text)\n",
        "        y = d[1]\n",
        "        X1.append(x1)\n",
        "        X2.append(x2)\n",
        "        Y.append([y])\n",
        "        if len(X1) == self.batch_size or i == idxs[-1]:\n",
        "          X1 = seq_padding(X1)\n",
        "          X2 = seq_padding(X2)\n",
        "          Y = seq_padding(Y)\n",
        "          yield [X1, X2], Y[:, 0, :]\n",
        "          [X1, X2, Y] = [], [], []\n",
        "\n",
        "#计算top-k正确率,当预测值的前k个值中存在目标类别即认为预测正确                 \n",
        "def acc_top2(y_true, y_pred):\n",
        "  return top_k_categorical_accuracy(y_true, y_pred, k=2)\n",
        "\n",
        "def boolMap(arr):\n",
        "  if arr > 0.5:\n",
        "      return 1\n",
        "  else:\n",
        "      return 0\n",
        "\n",
        "#自定义计算每个epoch的F1，注意不是batch的F1\n",
        "class Metrics(Callback):\n",
        "  def __init__(self, filepath):\n",
        "    self.file_path = filepath\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    self.val_f1s = []\n",
        "    self.best_val_f1 = 0\n",
        "    self.val_recalls = []\n",
        "    self.val_precisions = []\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    val_predict = list(map(boolMap, self.model.predict([self.validation_data[0], self.validation_data[1]])))\n",
        "    val_targ = self.validation_data[2]\n",
        "    _val_f1 = f1_score(val_targ, val_predict)\n",
        "    _val_recall = recall_score(val_targ, val_predict)\n",
        "    _val_precision = precision_score(val_targ, val_predict)\n",
        "    self.val_f1s.append(_val_f1)\n",
        "    self.val_recalls.append(_val_recall)\n",
        "    self.val_precisions.append(_val_precision)\n",
        "    print(_val_f1, _val_precision, _val_recall)\n",
        "    print(\"max f1\")\n",
        "    print(max(self.val_f1s))\n",
        "    if _val_f1 > self.best_val_f1:\n",
        "      self.model.save_weights(self.file_path, overwrite=True)\n",
        "      self.best_val_f1 = _val_f1\n",
        "      print(\"best f1: {}\".format(self.best_val_f1))\n",
        "    else:\n",
        "      print(\"val f1: {}, but not the best f1\".format(_val_f1))\n",
        "    return\n",
        "\n",
        "f1metrics = Metrics(path)\n",
        "\n",
        "#计算每个batch的f1\n",
        "from keras import backend as K\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "  def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "  def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "  precision = precision(y_true, y_pred)\n",
        "  recall = recall(y_true, y_pred)\n",
        "  return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "#bert模型设置\n",
        "def build_bert(nclass):\n",
        "  global lr_rate\n",
        "  bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)  #加载预训练模型\n",
        "\n",
        "  for l in bert_model.layers:\n",
        "    l.trainable = True\n",
        "\n",
        "  x1_in = Input(shape=(None,))\n",
        "  x2_in = Input(shape=(None,))\n",
        "\n",
        "  x = bert_model([x1_in, x2_in])\n",
        "  x = Lambda(lambda x: x[:, 0])(x) # 取出[CLS]对应的向量用来做分类\n",
        "  p = Dense(nclass, activation='softmax')(x)\n",
        "\n",
        "  model = Model([x1_in, x2_in], p)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(lr_rate),    #用足够小的学习率\n",
        "                metrics=['accuracy', f1])#acc_top2\n",
        "\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "#交叉验证训练和测试模型\n",
        "def run_nocv(nfold, trn_data, val_data, data_labels, data_test, n_cls):\n",
        "  global er_patience\n",
        "  global lr_patience\n",
        "  global max_epochs\n",
        "  global f1metrics\n",
        "  test_model_pred = np.zeros((len(data_test), n_cls))\n",
        "  '''    \n",
        "  # 按照9:1的比例划分训练集和验证集\n",
        "  random_order = list( range(len(data)) )\n",
        "  np.random.shuffle(random_order)\n",
        "  train_data = [data[j] for i, j in enumerate(random_order) if i % 10 != 0]\n",
        "  valid_data = [data[j] for i, j in enumerate(random_order) if i % 10 == 0]\n",
        "  '''\n",
        "\n",
        "  model = build_bert(n_cls)\n",
        "  early_stopping = EarlyStopping(monitor= \"val_accuracy\", patience=er_patience) #早停法，防止过拟合#'val_accuracy'\n",
        "  plateau = ReduceLROnPlateau(monitor=\"val_accuracy\", verbose=1, mode='max', factor=0.5, patience=lr_patience) #当评价指标不在提升时，减少学习率#\"val_accuracy\"\n",
        "  checkpoint = ModelCheckpoint(path + \"/subs/ocemo.epoch{epoch:02d}_val_loss{val_loss:.4f}_val_acc{val_accuracy:.4f}_val_f1{val_f1:.4f}.hdf5\", monitor=\"val_accuracy\", verbose=2, save_best_only=True, mode='max', save_weights_only=True) #保存最好的模型#'val_acc'\n",
        "\n",
        "  train_D = data_generator(trn_data, shuffle=True)\n",
        "  valid_D = data_generator(val_data, shuffle=True)\n",
        "  test_D  = data_generator(data_test, shuffle=False)\n",
        "  \n",
        "  #模型训练\n",
        "  model.fit_generator(\n",
        "      train_D.__iter__(),\n",
        "      steps_per_epoch=len(train_D),\n",
        "      epochs=max_epochs,\n",
        "      validation_data=valid_D.__iter__(),\n",
        "      validation_steps=len(valid_D),\n",
        "      callbacks=[early_stopping, plateau, checkpoint],\n",
        "  )\n",
        "\n",
        "  test_model_pred = model.predict_generator(test_D.__iter__(), steps=len(test_D), verbose=1)\n",
        "  train_model_pred = model.predict(train_D.__iter__(), steps=len(train_D), verbose=1)\n",
        " \n",
        "  gc.collect()   #清理内存\n",
        "  del model\n",
        "  K.clear_session() #clear_session就是清除一个session\n",
        "\n",
        "  return test_model_pred, train_model_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtaruPNcig6R",
        "outputId": "6e116979-74c7-44be-a018-23fa706af1be"
      },
      "source": [
        "cvs = 5\n",
        "test_model_pred, train_model_pred = run_nocv(cvs, TRN_LIST, VAL_LIST, None, DATA_LIST_TEST, n_cls)\n",
        "\n",
        "preds_col_names = encode_label.inverse_transform( range(0,n_cls) )\n",
        "preds_tst_df = pd.DataFrame(test_model_pred)\n",
        "preds_trn_df = pd.DataFrame(train_model_pred)\n",
        "\n",
        "preds_tst_df.columns = preds_col_names\n",
        "preds_trn_df.columns = preds_col_names\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Functional)            (None, None, 1024)   324472832   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 1024)         0           model_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 7)            7175        lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 324,480,007\n",
            "Trainable params: 324,480,007\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4709/4709 [==============================] - 1409s 291ms/step - loss: 1.3455 - accuracy: 0.5059 - f1: 0.3868 - val_loss: 1.0977 - val_accuracy: 0.6063 - val_f1: 0.5655\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.60626, saving model to /content/drive/MyDrive/Colab_NLP/tianchi-multi-task-nlp/subs/ocemo.epoch01_val_loss1.0977_val_acc0.6063_val_f10.5655.hdf5\n",
            "Epoch 2/3\n",
            "4709/4709 [==============================] - 1354s 287ms/step - loss: 0.9977 - accuracy: 0.6417 - f1: 0.6004 - val_loss: 1.0856 - val_accuracy: 0.6183 - val_f1: 0.5859\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.60626 to 0.61829, saving model to /content/drive/MyDrive/Colab_NLP/tianchi-multi-task-nlp/subs/ocemo.epoch02_val_loss1.0856_val_acc0.6183_val_f10.5859.hdf5\n",
            "Epoch 3/3\n",
            "4709/4709 [==============================] - 1351s 287ms/step - loss: 0.8115 - accuracy: 0.7073 - f1: 0.6827 - val_loss: 1.1320 - val_accuracy: 0.6118 - val_f1: 0.5954\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.61829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 23s 58ms/step\n",
            "4709/4709 [==============================] - 280s 59ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "7lkAGvXwR_uO",
        "outputId": "9ba82a1c-8c21-478e-8a8b-ca5533d146ac"
      },
      "source": [
        "preds_tst_df.head(ocemo_testa.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>anger</th>\n",
              "      <th>disgust</th>\n",
              "      <th>fear</th>\n",
              "      <th>happiness</th>\n",
              "      <th>like</th>\n",
              "      <th>sadness</th>\n",
              "      <th>surprise</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.006839</td>\n",
              "      <td>0.049984</td>\n",
              "      <td>0.067279</td>\n",
              "      <td>0.012439</td>\n",
              "      <td>0.005056</td>\n",
              "      <td>0.857248</td>\n",
              "      <td>0.001156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.999689</td>\n",
              "      <td>0.000006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.114630</td>\n",
              "      <td>0.036727</td>\n",
              "      <td>0.007626</td>\n",
              "      <td>0.063425</td>\n",
              "      <td>0.478429</td>\n",
              "      <td>0.163667</td>\n",
              "      <td>0.135496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.001175</td>\n",
              "      <td>0.002376</td>\n",
              "      <td>0.121482</td>\n",
              "      <td>0.361577</td>\n",
              "      <td>0.510859</td>\n",
              "      <td>0.000716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.003547</td>\n",
              "      <td>0.003791</td>\n",
              "      <td>0.000868</td>\n",
              "      <td>0.960563</td>\n",
              "      <td>0.021864</td>\n",
              "      <td>0.005930</td>\n",
              "      <td>0.003437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>0.000271</td>\n",
              "      <td>0.000578</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.994515</td>\n",
              "      <td>0.002443</td>\n",
              "      <td>0.000918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>0.015257</td>\n",
              "      <td>0.243143</td>\n",
              "      <td>0.004671</td>\n",
              "      <td>0.672554</td>\n",
              "      <td>0.010390</td>\n",
              "      <td>0.050677</td>\n",
              "      <td>0.003309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>0.333023</td>\n",
              "      <td>0.120519</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.004108</td>\n",
              "      <td>0.016267</td>\n",
              "      <td>0.518600</td>\n",
              "      <td>0.006321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>0.075615</td>\n",
              "      <td>0.053657</td>\n",
              "      <td>0.004608</td>\n",
              "      <td>0.038493</td>\n",
              "      <td>0.147233</td>\n",
              "      <td>0.651833</td>\n",
              "      <td>0.028560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>0.045491</td>\n",
              "      <td>0.355958</td>\n",
              "      <td>0.006113</td>\n",
              "      <td>0.069139</td>\n",
              "      <td>0.005045</td>\n",
              "      <td>0.505170</td>\n",
              "      <td>0.013084</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1500 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         anger   disgust      fear  happiness      like   sadness  surprise\n",
              "0     0.006839  0.049984  0.067279   0.012439  0.005056  0.857248  0.001156\n",
              "1     0.000091  0.000046  0.000010   0.000045  0.000113  0.999689  0.000006\n",
              "2     0.114630  0.036727  0.007626   0.063425  0.478429  0.163667  0.135496\n",
              "3     0.001815  0.001175  0.002376   0.121482  0.361577  0.510859  0.000716\n",
              "4     0.003547  0.003791  0.000868   0.960563  0.021864  0.005930  0.003437\n",
              "...        ...       ...       ...        ...       ...       ...       ...\n",
              "1495  0.000271  0.000578  0.000085   0.001190  0.994515  0.002443  0.000918\n",
              "1496  0.015257  0.243143  0.004671   0.672554  0.010390  0.050677  0.003309\n",
              "1497  0.333023  0.120519  0.001163   0.004108  0.016267  0.518600  0.006321\n",
              "1498  0.075615  0.053657  0.004608   0.038493  0.147233  0.651833  0.028560\n",
              "1499  0.045491  0.355958  0.006113   0.069139  0.005045  0.505170  0.013084\n",
              "\n",
              "[1500 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlOMPsKLVW8B"
      },
      "source": [
        "#times_testa, ocemo_testa, ocnli_testa\n",
        "#每行，从每个任务对应的标签列中找出最大的概率对应的列名作为预测结果\n",
        "ocemo_preds = preds_tst_df.head(ocemo_testa.shape[0])\n",
        "ocemo_preds = ocemo_preds.eq(ocemo_preds.max(1), axis=0).dot(ocemo_preds.columns)\n",
        "\n",
        "ocemo_sub = ocemo_testa[['id']].copy()\n",
        "ocemo_sub['label'] = ocemo_preds.values\n",
        "ocemo_sub.to_json(path + \"/submission/sub/ocemo_predict.json\", orient='records', lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpuDy06OwBYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03fa3cf-5211-4287-a0ff-abbffc6d105f"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab_NLP/tianchi-multi-task-nlp/subs/\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab_NLP/tianchi-multi-task-nlp/subs\n",
            "total 8874986\n",
            "-rw------- 1 root root 1298195560 Jan  5 09:18 ocemo.epoch01_val_loss1.0977_val_acc0.6063_val_f10.5655.hdf5\n",
            "-rw------- 1 root root 1298195560 Jan  5 09:41 ocemo.epoch02_val_loss1.0856_val_acc0.6183_val_f10.5859.hdf5\n",
            "-rw------- 1 root root      46824 Jan  5 10:13 ocemo_predict_merge.json\n",
            "-rw------- 1 root root 1298179176 Jan  5 10:52 ocnli.epoch01_val_loss0.5746_val_acc0.7610_val_f10.7537.hdf5\n",
            "-rw------- 1 root root      37889 Dec 28 11:04 ocnli_predict.json\n",
            "-rw------- 1 root root     290662 Dec 28 09:13 prob_sub1.csv\n",
            "-rw------- 1 root root 1298228328 Dec 29 11:14 times.epoch02_val_loss1.1762_val_acc0.5795_val_f10.5606.hdf5\n",
            "-rw------- 1 root root 1298228328 Dec 29 09:31 times.epoch02_val_loss1.1852_val_acc0.5777_val_f10.5633.hdf5\n",
            "-rw------- 1 root root 1298228328 Dec 29 11:31 times.epoch03_val_loss1.1817_val_acc0.5833_val_f10.5755.hdf5\n",
            "-rw------- 1 root root 1298228328 Dec 24 05:45 times.epoch03_val_loss1.4469_val_acc0.5833_val_f10.5835.hdf5\n",
            "-rw------- 1 root root      40889 Jan  5 11:12 tnews_predict_final.json\n",
            "-rw------- 1 root root      40889 Dec 28 09:20 tnews_predict.json\n",
            "-rw------- 1 root root      40889 Dec 29 12:09 tnews_predict_merge.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z-TqR-XuuVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd733109-93e6-4107-89fd-e7d21a2fc751"
      },
      "source": [
        "# rebuild model as model2\n",
        "# simulating starting a new script\n",
        "model2 = build_bert(7)\n",
        "\n",
        "# load 3rd epoch weights\n",
        "#model2.load_weights('times.epoch03_val_loss1.4469_val_acc0.5833_val_f10.5835.hdf5')\n",
        "model2.load_weights('ocemo.epoch02_val_loss1.0856_val_acc0.6183_val_f10.5859.hdf5')\n",
        "test_D  = data_generator(DATA_LIST_TEST, shuffle=False)\n",
        "test_model_pred = model2.predict_generator(test_D.__iter__(), steps=len(test_D), verbose=1)\n",
        "\n",
        "preds_col_names = encode_label.inverse_transform( range(0,n_cls) )\n",
        "preds_tst_df = pd.DataFrame(test_model_pred)\n",
        "\n",
        "preds_tst_df.columns = preds_col_names\n",
        "\n",
        "preds_tst_df.to_csv('/content/drive/MyDrive/Colab_NLP/tianchi-multi-task-nlp/submission/sub/prob_ocemo_sub_final.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Functional)            (None, None, 1024)   324472832   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 1024)         0           model_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 7)            7175        lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 324,480,007\n",
            "Trainable params: 324,480,007\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 36s 102ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nctVwtXVlKV3",
        "outputId": "3f52d7ca-a7e3-43fd-a5ed-a26df1f0eec7"
      },
      "source": [
        "%cd ~\n",
        "%cd /content/drive/MyDrive/Colab_NLP/tianchi-multi-task-nlp/submission/sub\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content/drive/MyDrive/Colab_NLP/tianchi-multi-task-nlp/submission/sub\n",
            "total 1348\n",
            "-rw------- 1 root root  46961 Jan  5 10:09 ocemo_predict.json\n",
            "-rw------- 1 root root  37889 Dec 28 11:07 ocnli_predict.json\n",
            "-rw------- 1 root root 122928 Jan  5 10:13 prob_ocemo_sub.csv\n",
            "-rw------- 1 root root 121289 Jan  5 11:17 prob_ocemo_sub_final.csv\n",
            "-rw------- 1 root root  53593 Dec 28 10:53 prob_ocnli_sub.csv\n",
            "-rw------- 1 root root 290662 Dec 27 09:47 prob_sub1.csv\n",
            "-rw------- 1 root root 291309 Jan  5 11:10 prob_sub.csv\n",
            "-rw------- 1 root root 290670 Dec 29 11:35 prob_sub_tnews_1229.csv\n",
            "-rw------- 1 root root  40889 Dec 29 11:35 tnews_predict_1229.json\n",
            "-rw------- 1 root root  40889 Dec 24 07:55 tnews_predict.json\n",
            "-rw------- 1 root root  40889 Dec 24 06:47 tnews_predict_sg.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB9ngsK7OJgC"
      },
      "source": [
        "# path = '/content/drive/MyDrive/Colab_NLP/tianchi-multi-task-nlp/submission'\n",
        "preds_tst_df = pd.read_csv(\"prob_ocemo_sub_final.csv\") #+ pd.read_csv( \"/subs/prob_sub.csv\")\n",
        "\n",
        "\n",
        "#每行，从每个任务对应的标签列中找出最大的概率对应的列名作为预测结果\n",
        "ocemo_preds = preds_tst_df.head(ocemo_testa.shape[0])\n",
        "ocemo_preds = ocemo_preds.eq(ocemo_preds.max(1), axis=0).dot(ocemo_preds.columns)\n",
        "\n",
        "ocemo_sub = ocemo_testa[['id']].copy()\n",
        "ocemo_sub['label'] = ocemo_preds.values\n",
        "ocemo_sub.to_json(path + \"/subs/ocemo_predict_final.json\", orient='records', lines=True)\n",
        "\n",
        "# ocemo_sub = ocemo_testa[['id']].copy()\n",
        "# ocemo_sub['label'] = ocemo_preds.values\n",
        "# ocemo_sub.to_json(path + \"/subs/ocemotion_predict.json\", orient='records', lines=True)\n",
        "\n",
        "# ocnli_sub = ocnli_testa[['id']].copy()\n",
        "# ocnli_sub['label'] = ocnli_preds.values\n",
        "# ocnli_sub.to_json(path + \"/subs/ocnli_predict.json\", orient='records', lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}